{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG Comparison\n",
    "\n",
    "Compare and summarize two YouTube videos using RAG with metadata-aware retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import unicodedata\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional, Dict, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    TranscriptsDisabled,\n",
    "    NoTranscriptFound,\n",
    ")\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from test2 import open_router_model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_classic.chains.query_constructor.base import (\n",
    "    AttributeInfo,\n",
    "    load_query_constructor_runnable,\n",
    ")\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"ollama\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"langchain\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ep = HuggingFaceEndpoint(repo_id=\"XiaomiMiMo/MiMo-V2-Flash\", temperature=0.5)\n",
    "model_hf = ChatHuggingFace(llm=model_ep)\n",
    "model_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0.5)\n",
    "model_google = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeOllamaEmbeddings(OllamaEmbeddings):\n",
    "    def _sanitize_vector(self, vec):\n",
    "        sanitized = []\n",
    "\n",
    "        def _flatten(x):\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                for el in x:\n",
    "                    yield from _flatten(el)\n",
    "            else:\n",
    "                yield x\n",
    "\n",
    "        for v in _flatten(vec):\n",
    "            try:\n",
    "                fv = float(v)\n",
    "                if not math.isfinite(fv):\n",
    "                    sanitized.append(0.0)\n",
    "                else:\n",
    "                    sanitized.append(fv)\n",
    "            except Exception:\n",
    "                sanitized.append(0.0)\n",
    "        return sanitized\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        raw = super().embed_documents(texts)\n",
    "        return [self._sanitize_vector(v) for v in raw]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                raw = super().embed_query(text)\n",
    "                return self._sanitize_vector(raw)\n",
    "            except Exception as e:\n",
    "                logger.debug(\"embed_query attempt %d failed: %s\", attempt + 1, e)\n",
    "                time.sleep(0.2 * (attempt + 1))\n",
    "        logger.warning(\"embed_query failed twice; returning safe zero vector\")\n",
    "        return [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SafeOllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "SUPPORTED_LANGS = [\n",
    "    \"en\", \"hi\", \"es\", \"fr\", \"de\", \"zh-Hans\", \"zh-Hant\", \"ja\", \"ko\", \"ru\", \"pt\", \"it\", \"ar\", \"tr\", \"vi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(url_or_id: str) -> str:\n",
    "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
    "    m = re.search(pattern, url_or_id)\n",
    "    return m.group(1) if m else url_or_id\n",
    "\n",
    "def parse_published_at(iso_ts: Optional[str]) -> str:\n",
    "    if not iso_ts:\n",
    "        return \"Unknown\"\n",
    "    try:\n",
    "        if iso_ts.endswith(\"Z\"):\n",
    "            iso_ts = iso_ts.replace(\"Z\", \"+00:00\")\n",
    "        return datetime.fromisoformat(iso_ts).date().isoformat()\n",
    "    except Exception:\n",
    "        try:\n",
    "            return datetime.strptime(iso_ts, \"%Y-%m-%dT%H:%M:%SZ\").date().isoformat()\n",
    "        except Exception:\n",
    "            return \"Unknown\"\n",
    "\n",
    "def sec_to_mmss(s: int) -> str:\n",
    "    m = s // 60\n",
    "    sec = s % 60\n",
    "    return f\"{m:02d}:{sec:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. YouTube Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_video_metadata(video_id: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        if not api_key:\n",
    "            logger.warning(\"GOOGLE_API_KEY not found; returning placeholder metadata.\")\n",
    "            return {\n",
    "                \"video_id\": video_id,\n",
    "                \"title\": \"Unknown\",\n",
    "                \"channel\": \"Unknown\",\n",
    "                \"date\": \"Unknown\",\n",
    "                \"description\": \"\",\n",
    "            }\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        response = youtube.videos().list(part=\"snippet\", id=video_id).execute()\n",
    "        items = response.get(\"items\", [])\n",
    "        if not items:\n",
    "            return {\n",
    "                \"video_id\": video_id,\n",
    "                \"title\": \"Unknown\",\n",
    "                \"channel\": \"Unknown\",\n",
    "                \"date\": \"Unknown\",\n",
    "                \"description\": \"\",\n",
    "            }\n",
    "        snippet = items[0].get(\"snippet\", {})\n",
    "        title = snippet.get(\"title\", \"Unknown\")\n",
    "        channel = snippet.get(\"channelTitle\", \"Unknown\")\n",
    "        published_at = parse_published_at(snippet.get(\"publishedAt\"))\n",
    "        description = snippet.get(\"description\", \"\")\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": title,\n",
    "            \"channel\": channel,\n",
    "            \"date\": published_at,\n",
    "            \"description\": description,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error fetching metadata: %s\", e)\n",
    "        return {\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": \"Unknown\",\n",
    "            \"channel\": \"Unknown\",\n",
    "            \"date\": \"Unknown\",\n",
    "            \"description\": \"\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_transcript_fragments(video_id: str):\n",
    "    api = YouTubeTranscriptApi()\n",
    "    try:\n",
    "        return api.fetch(video_id, languages=SUPPORTED_LANGS)\n",
    "    except TranscriptsDisabled:\n",
    "        logger.warning(f\"Transcripts disabled for {video_id}\")\n",
    "        return []\n",
    "    except NoTranscriptFound:\n",
    "        logger.warning(f\"No transcript for {video_id}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error fetching transcript: %s\", e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transcript Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_video_to_chunks(video_id: str) -> Tuple[List[Document], int]:\n",
    "    fragments = fetch_transcript_fragments(video_id)\n",
    "    fragment_docs = []\n",
    "    for fragment in fragments:\n",
    "        fragment_docs.append(\n",
    "            Document(\n",
    "                page_content=fragment.text,\n",
    "                metadata={\n",
    "                    \"video_id\": video_id,\n",
    "                    \"start\": int(fragment.start),\n",
    "                    \"end\": int(fragment.start + fragment.duration),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    total_chars = sum(len(d.page_content) for d in fragment_docs)\n",
    "    logger.info(f\"Total Transcript Length for {video_id}: {total_chars}\")\n",
    "\n",
    "    if not fragment_docs:\n",
    "        return [], 5\n",
    "\n",
    "    target_size = max(600, min(1200, int(total_chars / 50)))\n",
    "    overlap = int(target_size * 0.15)\n",
    "\n",
    "    raw_docs = []\n",
    "    current_content = []\n",
    "    current_start = 0\n",
    "    current_len = 0\n",
    "    for i, doc in enumerate(fragment_docs):\n",
    "        if not current_content:\n",
    "            current_start = doc.metadata.get(\"start\", 0)\n",
    "        current_content.append(doc.page_content)\n",
    "        current_len += len(doc.page_content)\n",
    "        current_end = doc.metadata.get(\"end\", 0)\n",
    "        if current_len >= target_size or i == len(fragment_docs) - 1:\n",
    "            timestamp_label = f\"[Timestamp: {int(current_start)}s] \"\n",
    "            raw_docs.append(\n",
    "                Document(\n",
    "                    page_content=timestamp_label + \" \".join(current_content),\n",
    "                    metadata={\n",
    "                        \"video_id\": video_id,\n",
    "                        \"start\": int(current_start),\n",
    "                        \"end\": int(current_end),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            current_content = []\n",
    "            current_len = 0\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=target_size, chunk_overlap=overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "    num_chunks = len(chunks)\n",
    "    if num_chunks < 20:\n",
    "        dynamic_k = min(num_chunks, 5)\n",
    "    else:\n",
    "        dynamic_k = max(5, min(10, int(math.log2(num_chunks) * 1.5)))\n",
    "\n",
    "    logger.info(\n",
    "        \"Video %s: chunks=%d, chunk_size=%d, dynamic_k=%d\",\n",
    "        video_id,\n",
    "        num_chunks,\n",
    "        target_size,\n",
    "        dynamic_k,\n",
    "    )\n",
    "    return chunks, dynamic_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chunk Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_chunks_for_embeddings(chunks: List[Document]) -> List[Document]:\n",
    "    validated_chunks: List[Document] = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        content = (chunk.page_content or \"\").strip()\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        content = \"\".join(ch for ch in content if unicodedata.category(ch)[0] != \"C\")\n",
    "        content = \" \".join(content.split())\n",
    "        if len(content) < 15:\n",
    "            continue\n",
    "\n",
    "        content = unicodedata.normalize(\"NFKC\", content)\n",
    "\n",
    "        success = False\n",
    "        vec = None\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                vec = embeddings.embed_query(content)\n",
    "                flat = []\n",
    "\n",
    "                def _flatten(x):\n",
    "                    if isinstance(x, (list, tuple)):\n",
    "                        for el in x:\n",
    "                            _flatten(el)\n",
    "                    else:\n",
    "                        flat.append(float(x))\n",
    "\n",
    "                _flatten(vec)\n",
    "                if not flat:\n",
    "                    raise ValueError(\"empty embedding\")\n",
    "                if any(not math.isfinite(v) for v in flat):\n",
    "                    raise ValueError(\"embedding contains NaN/Inf\")\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.debug(\n",
    "                    \"Embedding attempt %d failed for chunk start=%s: %s\",\n",
    "                    attempt + 1,\n",
    "                    chunk.metadata.get(\"start\"),\n",
    "                    e,\n",
    "                )\n",
    "                time.sleep(0.15 * (attempt + 1))\n",
    "\n",
    "        if success:\n",
    "            validated_chunks.append(Document(page_content=content, metadata=chunk.metadata))\n",
    "        else:\n",
    "            logger.debug(\"Embedding failed for chunk start=%s -> merging/skip\", chunk.metadata.get(\"start\"))\n",
    "            if validated_chunks:\n",
    "                prev = validated_chunks[-1]\n",
    "                merged = prev.page_content + \" \" + content\n",
    "                validated_chunks[-1] = Document(page_content=merged, metadata=prev.metadata)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    logger.info(\"✅ Validated %d/%d chunks for embedding\", len(validated_chunks), max(1, len(chunks)))\n",
    "    return validated_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vectorstore Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore_for_video(video_id: str, chunks: List[Document], collection_name: Optional[str] = None) -> Optional[Chroma]:\n",
    "    if collection_name is None:\n",
    "        collection_name = f\"youtube-transcript-{video_id}\"\n",
    "    for c in chunks:\n",
    "        c.metadata.setdefault(\"video_id\", video_id)\n",
    "\n",
    "    validated = validate_chunks_for_embeddings(chunks)\n",
    "\n",
    "    if not validated:\n",
    "        logger.warning(\"No valid chunks after validation for video %s — skipping vectorstore creation.\", video_id)\n",
    "        return None\n",
    "\n",
    "    vector_store = Chroma.from_documents(validated, embeddings, collection_name=collection_name)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Query Constructor & Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"start\",\n",
    "        description=\"The start time of the video segment in seconds (integer).\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"end\",\n",
    "        description=\"The end time of the video segment in seconds (integer).\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"video_id\",\n",
    "        description=\"The unique YouTube video identifier.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Transcript segments from a YouTube video\"\n",
    "\n",
    "query_constructor = load_query_constructor_runnable(\n",
    "    llm=open_router_model,\n",
    "    document_contents=document_content_description,\n",
    "    attribute_info=metadata_field_info,\n",
    ")\n",
    "\n",
    "def build_self_query_retriever(vectorstore: Chroma, dynamic_k: int, verbose: bool = False) -> SelfQueryRetriever:\n",
    "    retriever = SelfQueryRetriever(\n",
    "        query_constructor=query_constructor,\n",
    "        vectorstore=vectorstore,\n",
    "        structured_query_translator=ChromaTranslator(),\n",
    "        search_kwargs={\"k\": dynamic_k},\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieval Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_cache = {}\n",
    "retrieval_locks = defaultdict(threading.Lock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quality Detection & Evidence Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_low_quality_text(s: str, min_len: int = 15) -> bool:\n",
    "    if not s:\n",
    "        return True\n",
    "    txt = s.strip()\n",
    "    if len(txt) < min_len:\n",
    "        return True\n",
    "\n",
    "    tokens = txt.split()\n",
    "    if not tokens:\n",
    "        return True\n",
    "\n",
    "    one_char = sum(1 for t in tokens if len(t) == 1)\n",
    "    if one_char / max(1, len(tokens)) > 0.25:\n",
    "        return True\n",
    "\n",
    "    non_alnum = sum(1 for t in tokens if not any(ch.isalpha() or ch.isdigit() for ch in t))\n",
    "    if non_alnum / max(1, len(tokens)) > 0.4:\n",
    "        return True\n",
    "\n",
    "    if re.search(r\"[,.\\?\\!]{3,}\", txt):\n",
    "        return True\n",
    "\n",
    "    numeric_tokens = sum(1 for t in tokens if re.fullmatch(r\"[\\d,.\\-]+\", t))\n",
    "    if numeric_tokens / max(1, len(tokens)) > 0.6:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def format_evidence(docs: List[Document], filter_low_quality: bool = True) -> Tuple[str, Dict[str, int]]:\n",
    "    if not docs:\n",
    "        return \"No transcript evidence found.\", {\"kept\": 0, \"dropped\": 0}\n",
    "\n",
    "    kept_lines = []\n",
    "    dropped = 0\n",
    "    kept = 0\n",
    "\n",
    "    for d in docs:\n",
    "        content = d.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if filter_low_quality and is_low_quality_text(content):\n",
    "            dropped += 1\n",
    "            continue\n",
    "        s = int(d.metadata.get(\"start\", 0))\n",
    "        ts = sec_to_mmss(s)\n",
    "        quote = content\n",
    "        if len(quote) > 280:\n",
    "            quote = quote[:277] + \"...\"\n",
    "        kept_lines.append(f\"[{ts}] {quote}\")\n",
    "        kept += 1\n",
    "\n",
    "    if kept == 0:\n",
    "        return \"No good transcript evidence found (most retrieved segments were low-quality).\", {\"kept\": kept, \"dropped\": dropped}\n",
    "\n",
    "    return \"\\n\".join(kept_lines), {\"kept\": kept, \"dropped\": dropped}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a helpful YouTube AI assistant. PRIMARY TASK: answer the USER QUESTION using ONLY the provided VIDEO CONTENT evidence and metadata.\n",
    "\n",
    "VIDEO CONTENT:\n",
    "{context}\n",
    "\n",
    "CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- If you include a timestamped quote, append a source link like: Source: https://youtu.be/{video_id}?t={seconds}s\n",
    "- If transcript evidence is missing for the requested fact, explicitly say \"Not found in video transcript or metadata.\"\n",
    "- Keep responses brief, factual, and grounded in the evidence or metadata only.\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\", \"video_id\", \"seconds\", \"chat_history\", \"video_summary\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_PROMPT = \"\"\"\n",
    "You are an expert YouTube comparison analyst. The user asked: {user_question}\n",
    "\n",
    "Before answering:\n",
    "- Inspect METADATA_A and METADATA_B and the provided evidence blocks.\n",
    "- For each video, INFER a short \"Channel/Topic focus\" from the title + description + channel name (1 short line). If uncertain, say \"unknown\".\n",
    "- Use ONLY the provided metadata and evidence. Do NOT use external knowledge.\n",
    "\n",
    "METADATA_A:\n",
    "{metadata_a}\n",
    "\n",
    "METADATA_B:\n",
    "{metadata_b}\n",
    "\n",
    "VIDEO A EVIDENCE (top retrieved chunks):\n",
    "{evidence_a}\n",
    "\n",
    "VIDEO B EVIDENCE (top retrieved chunks):\n",
    "{evidence_b}\n",
    "\n",
    "GUIDELINES:\n",
    "1) Start with a 1-3 sentence SHORT ANSWER that directly responds to the user's question.\n",
    "2) If user asked \"which is better / which to study / which is more relevant\":\n",
    "   - Provide a DECISION block with:\n",
    "     - preferred_video: A / B / TIE / INSUFFICIENT_DATA\n",
    "     - reasons: 3 concise bullets (at least one referencing metadata date or channel)\n",
    "     - evidence: 2 lines with [mm:ss] short quotes\n",
    "     - confidence: 0-100\n",
    "3) If user asked a factual question, answer strictly from evidence and include SOURCES with timestamps.\n",
    "4) ALWAYS mention missing or noisy transcripts and whether you relied on metadata only.\n",
    "5) Provide \"Channel focus — Video A: ...\" and \"Channel focus — Video B: ...\" near the top.\n",
    "6) If a video is judged educational (lecture/tutorial), include STUDY_TIPS for that video (4-6 actionable bullets). Otherwise omit study tips.\n",
    "7) Tie-break rules: evidence presence -> recency (metadata.date) -> channel authority.\n",
    "8) Do not hallucinate. If requested facts are not in evidence/metadata, say \"Not found in video transcript or metadata.\"\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- Channel focus lines\n",
    "- SHORT ANSWER (1-3 sentences)\n",
    "- DECISION (if applicable)\n",
    "- SOURCES / EVIDENCE\n",
    "- STUDY_TIPS (if applicable)\n",
    "Keep responses concise and factual.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Dual Video Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dual_video_summary(chunks_a, chunks_b, metadata_a: Dict, metadata_b: Dict):\n",
    "    MAX_CHARS = 500000\n",
    "\n",
    "    def prepare_text(chunks, label):\n",
    "        total_text = \" \".join([c.page_content for c in chunks])\n",
    "        if len(total_text) > MAX_CHARS:\n",
    "            logger.info(\"%s is massive (%d chars). Using Smart Sampling...\", label, len(total_text))\n",
    "            step = len(total_text) // MAX_CHARS + 1\n",
    "            sampled_chunks = chunks[::step]\n",
    "            return \" \".join([c.page_content for c in sampled_chunks])\n",
    "        else:\n",
    "            logger.info(\"%s is standard size. Using full transcript...\", label)\n",
    "            return total_text\n",
    "\n",
    "    text_a = prepare_text(chunks_a, \"Video A\")\n",
    "    text_b = prepare_text(chunks_b, \"Video B\")\n",
    "\n",
    "    combined_prompt = f\"\"\"\n",
    "You are a professional YouTube learning analyst.\n",
    "\n",
    "We have TWO videos. Your task:\n",
    "\n",
    "1) Structured summary of Video A:\n",
    "   - 4-5 sentence overview\n",
    "   - Key takeaways (3-6 bullets)\n",
    "   - Mention main topics and style (tutorial/theory/demo)\n",
    "\n",
    "2) Structured summary of Video B:\n",
    "   - 4-5 sentence overview\n",
    "   - Key takeaways (3-6 bullets)\n",
    "   - Mention main topics and style\n",
    "\n",
    "3) Comparative overview:\n",
    "   - Major topic overlaps\n",
    "   - Key differences\n",
    "   - Tell user which is more recent (use metadata) if the vdos is about study or tech or any kind of learning stuff.\n",
    "\n",
    "In last invite user to ask more questions related to the videos.\n",
    "\n",
    "METADATA_A:\n",
    "Title: {metadata_a.get(\"title\")}\n",
    "Channel: {metadata_a.get(\"channel\")}\n",
    "Date: {metadata_a.get(\"date\")}\n",
    "\n",
    "VIDEO A CONTENT:\n",
    "{text_a}\n",
    "\n",
    "METADATA_B:\n",
    "Title: {metadata_b.get(\"title\")}\n",
    "Channel: {metadata_b.get(\"channel\")}\n",
    "Date: {metadata_b.get(\"date\")}\n",
    "\n",
    "VIDEO B CONTENT:\n",
    "{text_b}\n",
    "\"\"\"\n",
    "    res = open_router_model.invoke(combined_prompt)\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Core Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_video(video_url_or_id: str, collection_name: Optional[str] = None) -> Dict:\n",
    "    video_id = extract_video_id(video_url_or_id)\n",
    "    metadata = fetch_video_metadata(video_id)\n",
    "    chunks, dynamic_k = ingest_video_to_chunks(video_id)\n",
    "    if not chunks:\n",
    "        logger.info(\"No chunks for video %s\", video_id)\n",
    "        return {\"video_id\": video_id, \"metadata\": metadata, \"chunks\": [], \"dynamic_k\": dynamic_k, \"vectorstore\": None}\n",
    "    collection = collection_name or f\"youtube-{video_id}\"\n",
    "    vectorstore = create_vectorstore_for_video(video_id, chunks, collection_name=collection)\n",
    "    return {\"video_id\": video_id, \"metadata\": metadata, \"chunks\": chunks, \"dynamic_k\": dynamic_k, \"vectorstore\": vectorstore}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_single(video_url_or_id: str, user_question: str, history: Optional[ChatMessageHistory] = None) -> str:\n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    processed = process_and_store_video(video_url_or_id)\n",
    "    video_id = processed[\"video_id\"]\n",
    "    metadata = processed[\"metadata\"]\n",
    "    chunks = processed[\"chunks\"]\n",
    "    dynamic_k = processed[\"dynamic_k\"]\n",
    "    vectorstore = processed[\"vectorstore\"]\n",
    "\n",
    "    if not chunks or vectorstore is None:\n",
    "        return f\"Transcript not available for video {video_id}. Please enable captions or provide a transcript.\"\n",
    "\n",
    "    retriever = build_self_query_retriever(vectorstore, dynamic_k, verbose=False)\n",
    "    try:\n",
    "        retrieved_docs = retriever.get_relevant_documents(user_question)\n",
    "    except Exception:\n",
    "        try:\n",
    "            retrieved_docs = retriever.invoke(user_question)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"SelfQueryRetriever retrieval failed: %s\", e)\n",
    "            return \"Error retrieving relevant documents.\"\n",
    "\n",
    "    evidence_text, stats = format_evidence(retrieved_docs, filter_low_quality=True)\n",
    "    logger.info(\"Evidence kept=%d dropped=%d\", stats[\"kept\"], stats[\"dropped\"])\n",
    "\n",
    "    if stats[\"kept\"] == 0:\n",
    "        title = metadata.get(\"title\", \"Unknown\")\n",
    "        channel = metadata.get(\"channel\", \"Unknown\")\n",
    "        date = metadata.get(\"date\", \"Unknown\")\n",
    "        desc = metadata.get(\"description\", \"\")\n",
    "\n",
    "        fallback_prompt = f\"\"\"\n",
    "You are an assistant. The user asked: \"{user_question}\"\n",
    "\n",
    "I attempted to search the video's transcript, but the transcript appears corrupted or missing.\n",
    "Use ONLY the video metadata below to respond honestly and helpfully. If the metadata does not contain the requested info, say you can't find it in the video.\n",
    "\n",
    "METADATA:\n",
    "title: {title}\n",
    "channel: {channel}\n",
    "date: {date}\n",
    "description: {desc}\n",
    "\n",
    "Task:\n",
    "- If the user's question is general (e.g., 'what does the video talk about?'), provide a short summary based on metadata (1-3 sentences).\n",
    "- If the user's question asks for a factual detail not present in metadata (e.g., 'who is Virat Kohli?'), explicitly say: 'Not found in video transcript or metadata.'\n",
    "- Keep the reply brief and do NOT hallucinate.\n",
    "\"\"\"\n",
    "        fallback_resp = open_router_model.invoke(fallback_prompt)\n",
    "        history.add_user_message(user_question)\n",
    "        history.add_ai_message(fallback_resp.content)\n",
    "        return fallback_resp.content\n",
    "\n",
    "    good_docs = [d for d in retrieved_docs if not is_low_quality_text(d.page_content)]\n",
    "    context_text = \"\\n\\n\".join([f\"[{d.metadata['start']}s]: {d.page_content}\" for d in good_docs])\n",
    "    seconds = int(good_docs[0].metadata[\"start\"]) if good_docs else 0\n",
    "\n",
    "    history_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-6:]])\n",
    "\n",
    "    rag_chain = RAG_PROMPT | open_router_model | StrOutputParser()\n",
    "    try:\n",
    "        result = rag_chain.invoke({\n",
    "            \"context\": context_text,\n",
    "            \"chat_history\": history_str,\n",
    "            \"question\": user_question,\n",
    "            \"video_id\": video_id,\n",
    "            \"seconds\": seconds,\n",
    "            \"video_summary\": metadata.get(\"title\", \"this video\"),\n",
    "        })\n",
    "        history.add_user_message(user_question)\n",
    "        history.add_ai_message(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.exception(\"RAG generation failed: %s\", e)\n",
    "        return \"Error generating an answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structured_docs(vectorstore_id: str, retriever, user_query: str, k: int):\n",
    "    cache_key = f\"{vectorstore_id}__{user_query}\"\n",
    "    if cache_key in retrieval_cache:\n",
    "        logger.debug(\"Retrieval cache hit for key=%s\", cache_key)\n",
    "        return retrieval_cache[cache_key][:k]\n",
    "\n",
    "    lock = retrieval_locks[cache_key]\n",
    "    acquired = lock.acquire(blocking=False)\n",
    "    if not acquired:\n",
    "        logger.debug(\"Another retrieval in progress for key=%s; attempting fast fallback\", cache_key)\n",
    "        if cache_key in retrieval_cache:\n",
    "            return retrieval_cache[cache_key][:k]\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(user_query)\n",
    "            seen = set()\n",
    "            merged = []\n",
    "            for d in docs:\n",
    "                key = (d.metadata.get(\"video_id\"), d.metadata.get(\"start\"))\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                merged.append(d)\n",
    "            retrieval_cache[cache_key] = merged[:k]\n",
    "            return retrieval_cache[cache_key]\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Fallback retrieval failed: %s\", e)\n",
    "            return []\n",
    "\n",
    "    try:\n",
    "        try:\n",
    "            docs = retriever.get_relevant_documents(user_query)\n",
    "        except Exception:\n",
    "            try:\n",
    "                docs = retriever.invoke(user_query)\n",
    "            except Exception as e:\n",
    "                logger.exception(\"SelfQueryRetriever failed: %s\", e)\n",
    "                docs = []\n",
    "\n",
    "        final = []\n",
    "        seen_keys = set()\n",
    "        for d in docs:\n",
    "            key = (d.metadata.get(\"video_id\"), int(d.metadata.get(\"start\", 0)))\n",
    "            if key in seen_keys:\n",
    "                continue\n",
    "            seen_keys.add(key)\n",
    "            final.append(d)\n",
    "            if len(final) >= k:\n",
    "                break\n",
    "\n",
    "        retrieval_cache[cache_key] = final\n",
    "        logger.debug(\"Cached retrieval for key=%s -> %d docs\", cache_key, len(final))\n",
    "        return final\n",
    "    finally:\n",
    "        lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_videos(video_a_url_or_id: str, video_b_url_or_id: str, user_question: str, top_k: int = 5, history: Optional[ChatMessageHistory] = None) -> str:\n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    proc_a = process_and_store_video(video_a_url_or_id, collection_name=f\"youtube-{extract_video_id(video_a_url_or_id)}\")\n",
    "    proc_b = process_and_store_video(video_b_url_or_id, collection_name=f\"youtube-{extract_video_id(video_b_url_or_id)}\")\n",
    "\n",
    "    meta_a = proc_a[\"metadata\"]\n",
    "    meta_b = proc_b[\"metadata\"]\n",
    "    chunks_a = proc_a[\"chunks\"]\n",
    "    chunks_b = proc_b[\"chunks\"]\n",
    "    vs_a = proc_a[\"vectorstore\"]\n",
    "    vs_b = proc_b[\"vectorstore\"]\n",
    "    k_a = proc_a[\"dynamic_k\"] or 5\n",
    "    k_b = proc_b[\"dynamic_k\"] or 5\n",
    "\n",
    "    if (not chunks_a or vs_a is None) and (not chunks_b or vs_b is None):\n",
    "        return \"INSUFFICIENT_DATA: Transcripts missing for both videos. Please provide videos with captions or transcripts.\"\n",
    "\n",
    "    retr_a = build_self_query_retriever(vs_a, k_a, verbose=False) if vs_a else None\n",
    "    retr_b = build_self_query_retriever(vs_b, k_b, verbose=False) if vs_b else None\n",
    "\n",
    "    vectorstore_id_a = f\"youtube-{extract_video_id(video_a_url_or_id)}\"\n",
    "    vectorstore_id_b = f\"youtube-{extract_video_id(video_b_url_or_id)}\"\n",
    "\n",
    "    docs_a = get_structured_docs(vectorstore_id_a, retr_a, user_question, top_k)\n",
    "    docs_b = get_structured_docs(vectorstore_id_b, retr_b, user_question, top_k)\n",
    "\n",
    "    evidence_a_text, stats_a = format_evidence(docs_a, filter_low_quality=True)\n",
    "    evidence_b_text, stats_b = format_evidence(docs_b, filter_low_quality=True)\n",
    "\n",
    "    metadata_a_str = (\n",
    "        f\"video_id: {meta_a.get('video_id')}\\n\"\n",
    "        f\"title: {meta_a.get('title')}\\n\"\n",
    "        f\"channel: {meta_a.get('channel')}\\n\"\n",
    "        f\"date: {meta_a.get('date')}\\n\"\n",
    "        f\"description: {meta_a.get('description','')[:400]}\\n\"\n",
    "        f\"(evidence_kept: {stats_a['kept']}, evidence_dropped: {stats_a['dropped']})\"\n",
    "    )\n",
    "    metadata_b_str = (\n",
    "        f\"video_id: {meta_b.get('video_id')}\\n\"\n",
    "        f\"title: {meta_b.get('title')}\\n\"\n",
    "        f\"channel: {meta_b.get('channel')}\\n\"\n",
    "        f\"date: {meta_b.get('date')}\\n\"\n",
    "        f\"description: {meta_b.get('description','')[:400]}\\n\"\n",
    "        f\"(evidence_kept: {stats_b['kept']}, evidence_dropped: {stats_b['dropped']})\"\n",
    "    )\n",
    "\n",
    "    note = \"\"\n",
    "    if stats_a[\"kept\"] == 0 or stats_b[\"kept\"] == 0:\n",
    "        note = \"NOTE: One or both videos have poor-quality transcripts (many segments were dropped). Rely more on metadata.\\n\\n\"\n",
    "\n",
    "    prompt_text = note + COMPARISON_PROMPT.format(\n",
    "        user_question=user_question,\n",
    "        metadata_a=metadata_a_str,\n",
    "        metadata_b=metadata_b_str,\n",
    "        evidence_a=evidence_a_text,\n",
    "        evidence_b=evidence_b_text,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        res = open_router_model.invoke(prompt_text)\n",
    "        history.add_user_message(user_question)\n",
    "        history.add_ai_message(res.content)\n",
    "        return res.content\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Comparison model invocation failed: %s\", e)\n",
    "        return \"Error: comparison model invocation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Intent Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouterModel(BaseModel):\n",
    "    route: Literal[\"SUMMARY\", \"RAG\", \"COMPARE\", \"DUAL_SUMMARY\"] = Field(\n",
    "        description=\"User intent: SUMMARY, RAG, COMPARE, DUAL_SUMMARY\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(user_query: str, history: Optional[ChatMessageHistory] = None, secondary_provided: bool = False) -> str:\n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "    recent_history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-4:]])\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=RouterModel)\n",
    "    router_instruction = \"\"\"\n",
    "You are an expert query router. Based on the conversation history and the new request,\n",
    "choose one intent: SUMMARY, RAG, COMPARE, or DUAL_SUMMARY.\n",
    "\n",
    "- SUMMARY: high-level overview of a single video.\n",
    "- RAG: specific question / timestamp retrieval about a single video.\n",
    "- COMPARE: comparison / decision between two videos.\n",
    "- DUAL_SUMMARY: summarize both videos.\n",
    "\n",
    "Context:\n",
    "SECONDARY_PROVIDED: {secondary}\n",
    "CONVERSATION_HISTORY:\n",
    "{history}\n",
    "NEW_REQUEST:\n",
    "{query}\n",
    "\n",
    "Return a JSON object matching the schema.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=router_instruction,\n",
    "        input_variables=[\"history\", \"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = prompt | open_router_model | parser\n",
    "\n",
    "    try:\n",
    "        intent_obj = chain.invoke({\"history\": recent_history, \"query\": user_query, \"secondary\": str(secondary_provided)})\n",
    "        return intent_obj.route\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Router LLM error: %s. Defaulting to RAG.\", e)\n",
    "        return \"RAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Agent Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_user_query(primary_video_url_or_id: str, secondary_video_url_or_id: str, user_query: str, history: Optional[ChatMessageHistory] = None) -> Dict[str, Union[str, dict]]:\n",
    "    if history is None:\n",
    "        history = ChatMessageHistory()\n",
    "\n",
    "    if not primary_video_url_or_id or not secondary_video_url_or_id:\n",
    "        return {\"intent\": \"ERROR\", \"response\": \"Both primary and secondary video URLs/IDs are required for this MVP.\", \"meta\": {}}\n",
    "\n",
    "    intent = get_intent(user_query, history=history, secondary_provided=True)\n",
    "    logger.info(\"Detected intent: %s\", intent)\n",
    "\n",
    "    if intent == \"DUAL_SUMMARY\":\n",
    "        proc_a = process_and_store_video(primary_video_url_or_id, collection_name=f\"youtube-{extract_video_id(primary_video_url_or_id)}\")\n",
    "        proc_b = process_and_store_video(secondary_video_url_or_id, collection_name=f\"youtube-{extract_video_id(secondary_video_url_or_id)}\")\n",
    "        chunks_a = proc_a[\"chunks\"]\n",
    "        chunks_b = proc_b[\"chunks\"]\n",
    "        meta_a = proc_a[\"metadata\"]\n",
    "        meta_b = proc_b[\"metadata\"]\n",
    "\n",
    "        if (not chunks_a or proc_a[\"vectorstore\"] is None) and (not chunks_b or proc_b[\"vectorstore\"] is None):\n",
    "            resp = \"INSUFFICIENT_DATA: Transcripts missing for both videos. Cannot summarize.\"\n",
    "            history.add_user_message(user_query)\n",
    "            history.add_ai_message(resp)\n",
    "            return {\"intent\": intent, \"response\": resp, \"meta\": {}}\n",
    "\n",
    "        dual_summary = get_dual_video_summary(chunks_a, chunks_b, meta_a, meta_b)\n",
    "        history.add_user_message(user_query)\n",
    "        history.add_ai_message(dual_summary)\n",
    "        return {\"intent\": intent, \"response\": dual_summary, \"meta\": {}}\n",
    "\n",
    "    if intent == \"COMPARE\":\n",
    "        comp_resp = compare_videos(primary_video_url_or_id, secondary_video_url_or_id, user_question=user_query, top_k=5, history=history)\n",
    "        return {\"intent\": intent, \"response\": comp_resp, \"meta\": {}}\n",
    "\n",
    "    q = user_query.lower()\n",
    "    chosen = primary_video_url_or_id\n",
    "    if any(tok in q for tok in [\"second\", \"video b\", \"video 2\", \"secondary\"]):\n",
    "        chosen = secondary_video_url_or_id\n",
    "\n",
    "    rag_resp = answer_question_single(chosen, user_query, history=history)\n",
    "    return {\"intent\": \"RAG\", \"response\": rag_resp, \"meta\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Chat Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"=== YouTube RAG & Compare Chat (MVP: two videos required) ===\")\n",
    "    while True:\n",
    "        primary = input(\"Primary video URL or ID: \").strip()\n",
    "        if primary:\n",
    "            break\n",
    "        print(\"Primary video is required.\")\n",
    "    while True:\n",
    "        secondary = input(\"Secondary video URL or ID (required): \").strip()\n",
    "        if secondary and secondary != primary:\n",
    "            break\n",
    "        if not secondary:\n",
    "            print(\"Secondary video is required.\")\n",
    "        else:\n",
    "            print(\"Secondary must be different from primary. Please enter another video.\")\n",
    "\n",
    "    print(\"\\nReady. Type your question about both videos. Type 'exit' to quit.\")\n",
    "    history = ChatMessageHistory()\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nUser: \").strip()\n",
    "        except EOFError:\n",
    "            break\n",
    "        if not query:\n",
    "            continue\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye.\")\n",
    "            break\n",
    "\n",
    "        out = handle_user_query(primary, secondary, query, history=history)\n",
    "        intent = out.get(\"intent\")\n",
    "        resp = out.get(\"response\")\n",
    "        print(f\"\\nAI ({intent}):\\n{resp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Run Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
