{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG System\n",
    "## A Retrieval-Augmented Generation System for YouTube Video Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    TranscriptsDisabled,\n",
    "    NoTranscriptFound,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from test2 import open_router_model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo, load_query_constructor_runnable\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "model_ep = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", temperature=0.2)\n",
    "model_hf = ChatHuggingFace(llm=model_ep)\n",
    "model_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0.5)\n",
    "model_google = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a - Indexing (Document Ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Supported Languages and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major languages supported by BGE-M3\n",
    "SUPPORTED_LANGS = [\n",
    "    'en', 'hi', 'es', 'fr', 'de', 'zh-Hans', 'zh-Hant', \n",
    "    'ja', 'ko', 'ru', 'pt', 'it', 'ar', 'tr', 'vi'\n",
    "]\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_link = input(\"Paste YouTube Link: \")\n",
    "# video_id = extract_video_id(user_link)\n",
    "video_id = \"Gfr50f6ZBvo\"\n",
    "\n",
    "api = YouTubeTranscriptApi()\n",
    "try:\n",
    "    transcript_list = api.fetch(video_id, languages=SUPPORTED_LANGS)\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available for this video.\")\n",
    "except NoTranscriptFound:\n",
    "    print(\"No transcript found for this video.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching transcript: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b - Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Transcript Fragments to Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each transcript fragment to a Document with its exact timestamp\n",
    "fragment_docs = []\n",
    "for fragment in transcript_list:\n",
    "    fragment_docs.append(\n",
    "        Document(\n",
    "            page_content=fragment.text,\n",
    "            metadata={\n",
    "                \"video_id\": video_id,\n",
    "                \"start\": int(fragment.start),\n",
    "                \"end\": int(fragment.start + fragment.duration),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Fragments While Preserving Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge fragments while preserving timestamps\n",
    "raw_docs = []\n",
    "current_chunk = \"\"\n",
    "current_start = 0\n",
    "current_end = 0\n",
    "\n",
    "for doc in fragment_docs:\n",
    "    # Start new chunk\n",
    "    if not current_chunk:\n",
    "        current_start = doc.metadata['start']\n",
    "    \n",
    "    current_chunk += doc.page_content + \" \"\n",
    "    current_end = doc.metadata['end']\n",
    "    \n",
    "    if len(current_chunk) >= 750 or doc == fragment_docs[-1]:\n",
    "        raw_docs.append(\n",
    "            Document(\n",
    "                page_content=current_chunk.strip(),\n",
    "                metadata={\n",
    "                    \"video_id\": video_id,\n",
    "                    \"start\": current_start,\n",
    "                    \"end\": current_end,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        current_chunk = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Recursive Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Splitting \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134358"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_cont = \"/n/n\".join(c.page_content for c in chunks)\n",
    "len(page_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "# vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks, \n",
    "    embeddings, \n",
    "    collection_name=\"youtube-transcript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieval (Query Constructor) Using Self-Querying Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metadata Field Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"start\",\n",
    "        description=\"The start time of the video segment in seconds (integer). \"\n",
    "                    \"Rule 1: If user asks for 'at 12:00', use (start <= 720). \"\n",
    "                    \"Rule 2: If user asks for 'after 12:00', use (start >= 720). \"\n",
    "                    \"Rule 3: ALWAYS convert minutes to seconds (min * 60).\",\n",
    "        type=\"integer\", \n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"end\",\n",
    "        description=\"The end time of the video segment in seconds (integer). \"\n",
    "                    \"Rule 1: If user asks for 'at 12:00', use (end >= 720).\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"video_id\",\n",
    "        description=\"The unique YouTube video identifier.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Transcript segments from a YouTube video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Query Constructor and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 175\n",
      "Dynamic k: 12\n"
     ]
    }
   ],
   "source": [
    "# Use the runnable with the custom prompt\n",
    "query_constructor = load_query_constructor_runnable(\n",
    "    llm=open_router_model,\n",
    "    document_contents=document_content_description,\n",
    "    attribute_info=metadata_field_info,\n",
    ")\n",
    "\n",
    "num_chunks = len(chunks)\n",
    "print(f\"Number of chunks: {num_chunks}\")\n",
    "# `dynamic k` which can be adjusted based on the number of chunks automatically\n",
    "dynamic_k = max(4, min(12, num_chunks // 10))\n",
    "print(f\"Dynamic k: {dynamic_k}\")\n",
    "\n",
    "# Initialize Retriever\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vector_store,\n",
    "    structured_query_translator=ChromaTranslator(),\n",
    "    search_kwargs={\"k\": dynamic_k},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Augmentation (Formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Document Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting retrieved docs with timestamps so that llm can understand the context with time\n",
    "def format_docs(retrieved_docs):\n",
    "    context_entries = []\n",
    "    for doc in retrieved_docs:\n",
    "        s = doc.metadata['start']\n",
    "        timestamp = f\"{s // 60}:{s % 60:02d}\"\n",
    "        context_entries.append(f\"[{timestamp}]: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(context_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing intent for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Intent Router \n",
    "class Router(BaseModel):\n",
    "    choice: Literal[\"SUMMARY\", \"RAG\"] = Field(\n",
    "        description=\"\"\"\n",
    "        SUMMARY: Choose this if the user wants an overview, key takeaways, \n",
    "        a list of main points, or a general description of the WHOLE video. \n",
    "        \n",
    "        RAG: Choose this if the user is asking a specific question, looking \n",
    "        for a particular fact, person, timestamp, or details about a \n",
    "        specific sub-topic within the video.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "router_chain = open_router_model.with_structured_output(Router)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Summary Stuffing or Sampling\n",
    "If the yt transcript is greater than 500,000 characters, it takes every 2nd or 3rd chunk to stay in the high-accuracy zone else it uses the full transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Function \n",
    "def get_universal_summary(chunks): \n",
    "    MAX_CHARS = 500000 \n",
    "    # In tokens it is equivalent to MAX_CHARS / 4 (1 token = 4 characters)\n",
    "    \n",
    "    total_text = \" \".join([c.page_content for c in chunks])\n",
    "    \n",
    "    if len(total_text) > MAX_CHARS:\n",
    "        print(f\"Video is massive ({len(total_text)} chars). Using Smart Sampling...\")\n",
    "        # it takes every 2nd or 3rd chunk to stay in the high-accuracy zone\n",
    "        step = len(total_text) // MAX_CHARS + 1\n",
    "        sampled_chunks = chunks[::step] \n",
    "        final_text = \" \".join([c.page_content for c in sampled_chunks])\n",
    "    else:\n",
    "        print(\"Video is standard size. Using full transcript...\")\n",
    "        final_text = total_text\n",
    "\n",
    "    res = open_router_model.invoke(f\"\"\"\n",
    "        Summarize this YouTube video professionally. \n",
    "        Provide a concise 4-5 sentence overview followed by key takeaways in bullet points and mention all the key titles covered in the video.\n",
    "        \n",
    "        VIDEO CONTENT:\n",
    "        {final_text}\n",
    "    \"\"\")\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful YouTube AI assistant. \n",
    "    Answer the question based on the video content and our conversation history.\n",
    "    \n",
    "    VIDEO CONTENT:\n",
    "    {context}\n",
    "\n",
    "    CHAT HISTORY:\n",
    "    {chat_history}\n",
    "\n",
    "    USER QUESTION: \n",
    "    {question}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Answer using ONLY the video content provided. \n",
    "    2. Refer to the content as \"the video\" or \"the speaker,\" NEVER \"the transcript\" or \"the context.\"\n",
    "    3. If the information is NOT in the context, say: \"This topic is not discussed in the provided transcript segments.\"\n",
    "    4. If the answer IS present, conclude with the source link: https://youtu.be/{video_id}?t={seconds}s\n",
    "    5. Do not provide the source link if the information is not found.\n",
    "    6. Respond in the same language as the USER QUESTION.\n",
    "    7. If the user asks you your personal opinion then respond with generic reply \"I'm sorry, but I can't assist with that.\"\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"question\", \"video_id\", \"seconds\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate(\n",
    "#     template=\"\"\"\n",
    "# You are a helpful Youtube AI assistant that answers **only** from the provided VIDEO/ARTICLE/TRANSCRIPT CONTENT and chat history.\n",
    "\n",
    "# CONTEXT:\n",
    "# {context}\n",
    "\n",
    "# CHAT HISTORY:\n",
    "# {chat_history}\n",
    "\n",
    "# USER QUESTION:\n",
    "# {question}\n",
    "\n",
    "# RULES (must follow):\n",
    "# 1. Use only the provided CONTEXT and CHAT HISTORY. Do not add outside facts.\n",
    "# 2. Produce structured output with these sections (exact labels):\n",
    "#    - Top-line summary: one short sentence describing the content.\n",
    "#    - Breakdown: an ordered list of up to 8 key segments. For each:\n",
    "#        * Title: short descriptive title.\n",
    "#        * Timestamp: [mm:ss-mm:ss] (if timestamps unavailable, show [segment #N]).\n",
    "#        * Speaker: name if present; otherwise \"Speaker unknown\".\n",
    "#        * Summary: 1–3 sentences grounded in the context.\n",
    "#        * Link: if a source URL is provided, append a link or the base URL and include numeric start seconds in parentheses.\n",
    "#    - Follow-up/opinion behavior:\n",
    "#        * If the user asks for the assistant's personal opinion, reply exactly:\n",
    "#          \"I cannot provide personal opinions. However, based on the provided content:\" \n",
    "#          then provide an evidence-grounded answer citing timestamps/segments.\n",
    "#    - If the requested information is not in CONTEXT, reply exactly:\n",
    "#        \"This topic is not discussed in the provided transcript segments.\"\n",
    "# 3. Always answer in the same language the user used.\n",
    "# 4. Do not invent speaker names. If speaker name cannot be determined from the context, use \"Speaker unknown\" and say which segments support the answer.\n",
    "# 5. Keep quoted excerpts short (<= 25 words).\n",
    "# 6. Output only the structured content described — no extra commentary.\n",
    "# 7. If the answer IS present, conclude with the source link: https://youtu.be/{video_id}?t={seconds}s\n",
    "# 8. Do not provide the source link if the information is not found and user is not talking about the video.\n",
    "\n",
    "# \"\"\",\n",
    "#     input_variables=[\"context\", \"question\", \"video_id\", \"seconds\", \"chat_history\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Chat History and Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(query, history):\n",
    "    # We take the last 2 messages to give the Router context\n",
    "    recent_history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-2:]])\n",
    "    \n",
    "    router_instruction = f\"\"\"\n",
    "    You are an expert query router. Based on the conversation history and the new user request, \n",
    "    determine if the user wants a broad overview (SUMMARY) or a specific detail/follow-up (SPECIFIC_QUESTION).\n",
    "\n",
    "    CONVERSATION HISTORY:\n",
    "    {recent_history}\n",
    "\n",
    "    NEW REQUEST: \n",
    "    {query}\n",
    "\n",
    "    Rules:\n",
    "    - If the request is a follow-up to a previous specific point, pick SPECIFIC_QUESTION.\n",
    "    - If the request asks for a general overview of the whole video, pick SUMMARY.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the structured output chain you already defined\n",
    "    intent_obj = router_chain.invoke(router_instruction)\n",
    "    return intent_obj.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Video processed. I'm ready!\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Router\nchoice\n  Field required [type=missing, input_value={'route': 'SUMMARY'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     14\u001b[39m      \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m intent = \u001b[43mget_intent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m intent == \u001b[33m\"\u001b[39m\u001b[33mSUMMARY\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m summary_cache:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mget_intent\u001b[39m\u001b[34m(query, history)\u001b[39m\n\u001b[32m      5\u001b[39m router_instruction = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mYou are an expert query router. Based on the conversation history and the new user request, \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mdetermine if the user wants a broad overview (SUMMARY) or a specific detail/follow-up (SPECIFIC_QUESTION).\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33m- If the request asks for a general overview of the whole video, pick SUMMARY.\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Using the structured output chain you already defined\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m intent_obj = \u001b[43mrouter_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouter_instruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m intent_obj.choice\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3149\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3147\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3149\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3150\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3151\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1386\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1385\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1389\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1391\u001b[39m ):\n\u001b[32m   1392\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1358\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1353\u001b[39m     raw_response = (\n\u001b[32m   1354\u001b[39m         \u001b[38;5;28mself\u001b[39m.root_client.chat.completions.with_raw_response.parse(\n\u001b[32m   1355\u001b[39m             **payload\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m   1357\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1358\u001b[39m     response = \u001b[43mraw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1360\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\_legacy_response.py:139\u001b[39m, in \u001b[36mLegacyAPIResponse.parse\u001b[39m\u001b[34m(self, to)\u001b[39m\n\u001b[32m    137\u001b[39m parsed = \u001b[38;5;28mself\u001b[39m._parse(to=to)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m._options.post_parser):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     parsed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[32m    142\u001b[39m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m.request_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:178\u001b[39m, in \u001b[36mCompletions.parse.<locals>.parser\u001b[39m\u001b[34m(raw_completion)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_tools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_completion_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py:146\u001b[39m, in \u001b[36mparse_chat_completion\u001b[39m\u001b[34m(response_format, input_tools, chat_completion)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    137\u001b[39m                 tool_calls.append(tool_call)\n\u001b[32m    139\u001b[39m     choices.append(\n\u001b[32m    140\u001b[39m         construct_type_unchecked(\n\u001b[32m    141\u001b[39m             type_=cast(Any, ParsedChoice)[solve_response_format_t(response_format)],\n\u001b[32m    142\u001b[39m             value={\n\u001b[32m    143\u001b[39m                 **choice.to_dict(),\n\u001b[32m    144\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m    145\u001b[39m                     **message.to_dict(),\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mparsed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmaybe_parse_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    150\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m\"\u001b[39m: tool_calls \u001b[38;5;28;01mif\u001b[39;00m tool_calls \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    151\u001b[39m                 },\n\u001b[32m    152\u001b[39m             },\n\u001b[32m    153\u001b[39m         )\n\u001b[32m    154\u001b[39m     )\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    157\u001b[39m     ParsedChatCompletion[ResponseFormatT],\n\u001b[32m    158\u001b[39m     construct_type_unchecked(\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m     ),\n\u001b[32m    165\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py:199\u001b[39m, in \u001b[36mmaybe_parse_content\u001b[39m\u001b[34m(response_format, message)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmaybe_parse_content\u001b[39m(\n\u001b[32m    194\u001b[39m     *,\n\u001b[32m    195\u001b[39m     response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT] | ResponseFormatParam | Omit,\n\u001b[32m    196\u001b[39m     message: ChatCompletionMessage | ParsedChatCompletionMessage[\u001b[38;5;28mobject\u001b[39m],\n\u001b[32m    197\u001b[39m ) -> ResponseFormatT | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_rich_response_format(response_format) \u001b[38;5;129;01mand\u001b[39;00m message.content \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message.refusal:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\lib\\_parsing\\_completions.py:262\u001b[39m, in \u001b[36m_parse_content\u001b[39m\u001b[34m(response_format, content)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_parse_content\u001b[39m(response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT], content: \u001b[38;5;28mstr\u001b[39m) -> ResponseFormatT:\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_basemodel_type(response_format):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseFormatT, \u001b[43mmodel_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass_like_type(response_format):\n\u001b[32m    265\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m PYDANTIC_V1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\openai\\_compat.py:171\u001b[39m, in \u001b[36mmodel_parse_json\u001b[39m\u001b[34m(model, data)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m PYDANTIC_V1:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.parse_raw(data)  \u001b[38;5;66;03m# pyright: ignore[reportDeprecated]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GenAIProjects\\venv\\Lib\\site-packages\\pydantic\\main.py:766\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, extra, context, by_alias, by_name)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    762\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    763\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    764\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Router\nchoice\n  Field required [type=missing, input_value={'route': 'SUMMARY'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "parser = StrOutputParser()\n",
    "rag_chain = prompt | open_router_model | parser\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "summary_cache = None \n",
    "\n",
    "print(\"\\nAI: Video processed. I'm ready!\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nUser: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "         break\n",
    "\n",
    "    intent = get_intent(query, history)\n",
    "    \n",
    "    if intent == \"SUMMARY\":\n",
    "        if not summary_cache:\n",
    "            summary_cache = get_universal_summary(chunks)\n",
    "        context_text = summary_cache\n",
    "        timestamp = 0\n",
    "    else:\n",
    "        # RAG Mode (Specific Fact Finding)\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        sorted_docs = sorted(retrieved_docs, key=lambda x: x.metadata.get('start', 0))\n",
    "        context_text = \"\\n\\n\".join([f\"[{d.metadata['start']}s]: {d.page_content}\" for d in sorted_docs])\n",
    "        timestamp = sorted_docs[0].metadata['start'] if sorted_docs else 0\n",
    "\n",
    "    # CALL 3: Final Answer Generation (keeping last 6 messages in the chat history)\n",
    "    history_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-6:]])\n",
    "\n",
    "    result = rag_chain.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"question\": query,\n",
    "        \"video_id\": video_id,\n",
    "        \"seconds\": timestamp,\n",
    "        \"chat_history\": history_str \n",
    "    })\n",
    "\n",
    "    history.add_user_message(query)\n",
    "    history.add_ai_message(result)\n",
    "    \n",
    "    print(\"\\nUser: \" + query)\n",
    "    print(f\"\\nAI ({intent} mode): {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "# question = \"is the topic of origin of life discussed in this video? if yes then what was discussed\"\n",
    "# question = \"What does Demis say about aliens or life on other planets?\"\n",
    "# question = \"What is the fundamental problem that AlphaFold solved, and why is it significant for biology and medicine?\"\n",
    "# question = \"Who first articulated the protein folding problem, and when?\"\n",
    "# question = \"Does Demis Hassabis believe we are living in a computer game-like simulation, and what is his alternative view on understanding the universe?\"\n",
    "# question = \"What game does Demis consider the most impressive example of reinforcement learning in a computer game, and what was its core mechanic?\"\n",
    "# question = \"What specific topic is Demis Hassabis discussing right at (34:00) in the video?\"\n",
    "# question = \"Dr. Divyakirti ne evolutionary psychology ka use karke, men mein emotions suppress karne ke phenomenon ko kaise explain kiya hai?\"\n",
    "# question = \"Video mein, Dr. Divyakirti ne poverty scar hypothesis ke baare mein kya bataya? Aur unka personal experience is hypothesis se kaise align karta hai ya differ karta hai? \"\n",
    "question = \"Dr. Divyakirti ne fame aur popularity ko kaise dekha hai, especially jab negative incidents unse associate kiye jaate hain, even if it's a misunderstanding? Unhone is situation ko handle karne ke liye kaun sa analogy use kiya?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question) \n",
    "context_text = format_docs(retrieved_docs)\n",
    "first_timestamp = retrieved_docs[0].metadata['start'] if retrieved_docs else 0\n",
    "\n",
    "result = (prompt | open_router_model | parser).invoke({\n",
    "    \"context\": context_text,\n",
    "    \"question\": question,\n",
    "    \"video_id\": video_id,\n",
    "    \"seconds\": first_timestamp\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
