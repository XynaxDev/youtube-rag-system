{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG System\n",
    "## A Retrieval-Augmented Generation System for YouTube Video Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    TranscriptsDisabled,\n",
    "    NoTranscriptFound,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo, load_query_constructor_runnable\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAIProjects\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "model_ep = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", temperature=0.2)\n",
    "model_hf = ChatHuggingFace(llm=model_ep)\n",
    "model_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0.5)\n",
    "model_google = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a - Indexing (Document Ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Supported Languages and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major languages supported by BGE-M3\n",
    "SUPPORTED_LANGS = [\n",
    "    'en', 'hi', 'es', 'fr', 'de', 'zh-Hans', 'zh-Hant', \n",
    "    'ja', 'ko', 'ru', 'pt', 'it', 'ar', 'tr', 'vi'\n",
    "]\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_link = input(\"Paste YouTube Link: \")\n",
    "# video_id = extract_video_id(user_link)\n",
    "video_id = \"Gfr50f6ZBvo\"\n",
    "\n",
    "api = YouTubeTranscriptApi()\n",
    "try:\n",
    "    transcript_list = api.fetch(video_id, languages=SUPPORTED_LANGS)\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available for this video.\")\n",
    "except NoTranscriptFound:\n",
    "    print(\"No transcript found for this video.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching transcript: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b - Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Transcript Fragments to Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each transcript fragment to a Document with its exact timestamp\n",
    "fragment_docs = []\n",
    "for fragment in transcript_list:\n",
    "    fragment_docs.append(\n",
    "        Document(\n",
    "            page_content=fragment.text,\n",
    "            metadata={\n",
    "                \"video_id\": video_id,\n",
    "                \"start\": int(fragment.start),\n",
    "                \"end\": int(fragment.start + fragment.duration),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Fragments While Preserving Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge fragments while preserving timestamps\n",
    "raw_docs = []\n",
    "current_chunk = \"\"\n",
    "current_start = 0\n",
    "current_end = 0\n",
    "\n",
    "for doc in fragment_docs:\n",
    "    # Start new chunk\n",
    "    if not current_chunk:\n",
    "        current_start = doc.metadata['start']\n",
    "    \n",
    "    current_chunk += doc.page_content + \" \"\n",
    "    current_end = doc.metadata['end']\n",
    "    \n",
    "    if len(current_chunk) >= 750 or doc == fragment_docs[-1]:\n",
    "        raw_docs.append(\n",
    "            Document(\n",
    "                page_content=current_chunk.strip(),\n",
    "                metadata={\n",
    "                    \"video_id\": video_id,\n",
    "                    \"start\": current_start,\n",
    "                    \"end\": current_end,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        current_chunk = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Recursive Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Splitting \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134358"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_cont = \"/n/n\".join(c.page_content for c in chunks)\n",
    "len(page_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "# vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks, \n",
    "    embeddings, \n",
    "    collection_name=\"youtube-transcript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieval (Query Constructor) Using Self-Querying Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metadata Field Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"start\",\n",
    "        description=\"The start time of the video segment in seconds (integer). \"\n",
    "                    \"Rule 1: If user asks for 'at 12:00', use (start <= 720). \"\n",
    "                    \"Rule 2: If user asks for 'after 12:00', use (start >= 720). \"\n",
    "                    \"Rule 3: ALWAYS convert minutes to seconds (min * 60).\",\n",
    "        type=\"integer\", \n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"end\",\n",
    "        description=\"The end time of the video segment in seconds (integer). \"\n",
    "                    \"Rule 1: If user asks for 'at 12:00', use (end >= 720).\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"video_id\",\n",
    "        description=\"The unique YouTube video identifier.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Transcript segments from a YouTube video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Query Constructor and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 175\n",
      "Dynamic k: 12\n"
     ]
    }
   ],
   "source": [
    "# Use the runnable with the custom prompt\n",
    "query_constructor = load_query_constructor_runnable(\n",
    "    llm=model_google,\n",
    "    document_contents=document_content_description,\n",
    "    attribute_info=metadata_field_info,\n",
    ")\n",
    "\n",
    "num_chunks = len(chunks)\n",
    "print(f\"Number of chunks: {num_chunks}\")\n",
    "# `dynamic k` which can be adjusted based on the number of chunks automatically\n",
    "dynamic_k = max(4, min(12, num_chunks // 10))\n",
    "print(f\"Dynamic k: {dynamic_k}\")\n",
    "\n",
    "# Initialize Retriever\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vector_store,\n",
    "    structured_query_translator=ChromaTranslator(),\n",
    "    search_kwargs={\"k\": dynamic_k},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Augmentation (Formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Document Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting retrieved docs with timestamps so that llm can understand the context with time\n",
    "def format_docs(retrieved_docs):\n",
    "    context_entries = []\n",
    "    for doc in retrieved_docs:\n",
    "        s = doc.metadata['start']\n",
    "        timestamp = f\"{s // 60}:{s % 60:02d}\"\n",
    "        context_entries.append(f\"[{timestamp}]: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(context_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing intent for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Intent Router \n",
    "class Router(BaseModel):\n",
    "    choice: Literal[\"SUMMARY\", \"RAG\"] = Field(\n",
    "        description=\"\"\"\n",
    "        SUMMARY: Choose this if the user wants an overview, key takeaways, \n",
    "        a list of main points, or a general description of the WHOLE video. \n",
    "        \n",
    "        RAG: Choose this if the user is asking a specific question, looking \n",
    "        for a particular fact, person, timestamp, or details about a \n",
    "        specific sub-topic within the video.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "router_chain = model_google.with_structured_output(Router)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Summary Stuffing or Sampling\n",
    "If the yt transcript is greater than 500,000 characters, it takes every 2nd or 3rd chunk to stay in the high-accuracy zone else it uses the full transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Function \n",
    "def get_universal_summary(chunks): \n",
    "    MAX_CHARS = 500000 \n",
    "    # In tokens it is equivalent to MAX_CHARS / 4 (1 token = 4 characters)\n",
    "    \n",
    "    total_text = \" \".join([c.page_content for c in chunks])\n",
    "    \n",
    "    if len(total_text) > MAX_CHARS:\n",
    "        print(f\"Video is massive ({len(total_text)} chars). Using Smart Sampling...\")\n",
    "        # it takes every 2nd or 3rd chunk to stay in the high-accuracy zone\n",
    "        step = len(total_text) // MAX_CHARS + 1\n",
    "        sampled_chunks = chunks[::step] \n",
    "        final_text = \" \".join([c.page_content for c in sampled_chunks])\n",
    "    else:\n",
    "        print(\"Video is standard size. Using full transcript...\")\n",
    "        final_text = total_text\n",
    "\n",
    "    res = model_google.invoke(f\"\"\"\n",
    "        Summarize this YouTube video professionally. \n",
    "        Provide a concise 4-5 sentence overview followed by key takeaways in bullet points.\n",
    "        \n",
    "        VIDEO CONTENT:\n",
    "        {final_text}\n",
    "    \"\"\")\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful YouTube AI assistant. \n",
    "    Answer the question based on the video content and our conversation history.\n",
    "    \n",
    "    VIDEO CONTENT:\n",
    "    {context}\n",
    "\n",
    "    CHAT HISTORY:\n",
    "    {chat_history}\n",
    "\n",
    "    USER QUESTION: \n",
    "    {question}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Answer using ONLY the video content provided. \n",
    "    2. Refer to the content as \"the video\" or \"the speaker,\" NEVER \"the transcript\" or \"the context.\"\n",
    "    3. If the information is NOT in the context, say: \"This topic is not discussed in the provided transcript segments.\"\n",
    "    4. If the answer IS present, conclude with the source link: https://youtu.be/{video_id}?t={seconds}s\n",
    "    5. Do not provide a link if the information is not found.\n",
    "    6. Respond in the same language as the USER QUESTION.\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"question\", \"video_id\", \"seconds\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Chat History and Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(query, history):\n",
    "    # We take the last 2 messages to give the Router context\n",
    "    recent_history = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-2:]])\n",
    "    \n",
    "    router_instruction = f\"\"\"\n",
    "    You are an expert query router. Based on the conversation history and the new user request, \n",
    "    determine if the user wants a broad overview (SUMMARY) or a specific detail/follow-up (SPECIFIC_QUESTION).\n",
    "\n",
    "    CONVERSATION HISTORY:\n",
    "    {recent_history}\n",
    "\n",
    "    NEW REQUEST: \n",
    "    {query}\n",
    "\n",
    "    Rules:\n",
    "    - If the request is a follow-up to a previous specific point, pick SPECIFIC_QUESTION.\n",
    "    - If the request asks for a general overview of the whole video, pick SUMMARY.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using the structured output chain you already defined\n",
    "    intent_obj = router_chain.invoke(router_instruction)\n",
    "    return intent_obj.choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Video processed. I'm ready!\n",
      "Video is standard size. Using full transcript...\n",
      "\n",
      "AI (SUMMARY mode): The main topics about AI discussed in this video are:\n",
      "\n",
      "1. Rethinking Intelligence Benchmarks: The speaker argues that the Turing Test is an insufficient formal benchmark for AI and advocates for more rigorous, general tests that evaluate AI capabilities across a multitude of cognitive tasks to achieve true general intelligence.\n",
      "2. Games as Foundational for AI: DeepMind's early success was rooted in using games like Go as efficient, well-defined environments to develop and scale reinforcement learning algorithms, demonstrating AI's capacity for self-play and surpassing human performance.\n",
      "3. AlphaFold's Impact on Biology: The solution to the protein folding problem by AlphaFold 2 marks a significant scientific breakthrough, illustrating AI's transformative potential in biology, accelerating drug discovery, and paving the way for simulating complex biological systems like virtual cells.\n",
      "4. AI for Fundamental Science: DeepMind is applying AI to other grand scientific challenges, including controlling high-temperature plasma for nuclear fusion and modeling quantum mechanical electron behavior, aiming to use AI to advance our understanding of physics and the universe.\n",
      "5. Consciousness and AGI Development: The speaker suggests consciousness and intelligence are dissociable, advocating for the initial development of AI as non-conscious tools. He cautions against anthropomorphizing current AI systems and stresses the need for extensive research into interpretability and ethical guardrails before widespread deployment.\n",
      "6. Ethical Stewardship of AI: Emphasizing the immense power of AI, Hassabis calls for a multi-disciplinary approach involving philosophers, social scientists, and a wide array of voices to guide AI's development and deployment responsibly, ensuring it serves humanity's flourishing.\n",
      "7. Personal Purpose: Hassabis's ultimate motivation is to use AI as a tool to gain knowledge and understand the universe, believing this pursuit to be a core human virtue that fosters compassion and a deeper appreciation for existence.\n",
      "\n",
      "Source link: https://youtu.be/Gfr50f6ZBvo?t=0s\n",
      "\n",
      "AI (RAG mode): The 7th topic discussed in the video is about the speaker's personal purpose and motivation for developing AI.\n",
      "\n",
      "According to the video, the speaker believes that using AI as a tool to gain knowledge and understand the universe is a core human virtue that fosters compassion and a deeper appreciation for existence. They seem to be driven by a desire to make a positive impact on humanity and improve people's lives through their work in AI development.\n",
      "\n",
      "The speaker mentions that they hope AI will be used to help solve big challenges, such as energy and climate issues, and to cure diseases, ultimately leading to a world of \"radical abundance.\" They also believe that AI has the potential to help humanity flourish.\n",
      "\n",
      "Source link: https://youtu.be/Gfr50f6ZBVo?t=7148s\n",
      "\n",
      "AI (RAG mode): The speaker's opinion on this topic is that they believe it is important to remain humble and grounded, regardless of one's achievements or advancements in AI development. They mention that their best friends from their undergraduate days are still important to them, and that being a multi-disciplinary person helps to keep one humble.\n",
      "\n",
      "They also emphasize the importance of having a good team around you who are also ethical and grounded, as this can help to prevent the misuse of AI power.\n",
      "\n",
      "The speaker hopes that by developing AI responsibly, they will be able to use it to make a positive impact on humanity and improve people's lives.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "parser = StrOutputParser()\n",
    "rag_chain = prompt | model_ollama | parser\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "summary_cache = None \n",
    "\n",
    "print(\"\\nAI: Video processed. I'm ready!\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nUser: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "         break\n",
    "\n",
    "    intent = get_intent(query, history)\n",
    "    \n",
    "    if intent == \"SUMMARY\":\n",
    "        if not summary_cache:\n",
    "            summary_cache = get_universal_summary(chunks)\n",
    "        context_text = summary_cache\n",
    "        timestamp = 0\n",
    "    else:\n",
    "        # RAG Mode (Specific Fact Finding)\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        sorted_docs = sorted(retrieved_docs, key=lambda x: x.metadata.get('start', 0))\n",
    "        context_text = \"\\n\\n\".join([f\"[{d.metadata['start']}s]: {d.page_content}\" for d in sorted_docs])\n",
    "        timestamp = sorted_docs[0].metadata['start'] if sorted_docs else 0\n",
    "\n",
    "    # CALL 3: Final Answer Generation (keeping last 6 messages in the chat history)\n",
    "    history_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-6:]])\n",
    "\n",
    "    result = rag_chain.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"question\": query,\n",
    "        \"video_id\": video_id,\n",
    "        \"seconds\": timestamp,\n",
    "        \"chat_history\": history_str \n",
    "    })\n",
    "\n",
    "    history.add_user_message(query)\n",
    "    history.add_ai_message(result)\n",
    "\n",
    "    print(f\"\\nAI ({intent} mode): {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "# question = \"is the topic of origin of life discussed in this video? if yes then what was discussed\"\n",
    "# question = \"What does Demis say about aliens or life on other planets?\"\n",
    "# question = \"What is the fundamental problem that AlphaFold solved, and why is it significant for biology and medicine?\"\n",
    "# question = \"Who first articulated the protein folding problem, and when?\"\n",
    "# question = \"Does Demis Hassabis believe we are living in a computer game-like simulation, and what is his alternative view on understanding the universe?\"\n",
    "# question = \"What game does Demis consider the most impressive example of reinforcement learning in a computer game, and what was its core mechanic?\"\n",
    "# question = \"What specific topic is Demis Hassabis discussing right at (34:00) in the video?\"\n",
    "# question = \"Dr. Divyakirti ne evolutionary psychology ka use karke, men mein emotions suppress karne ke phenomenon ko kaise explain kiya hai?\"\n",
    "# question = \"Video mein, Dr. Divyakirti ne poverty scar hypothesis ke baare mein kya bataya? Aur unka personal experience is hypothesis se kaise align karta hai ya differ karta hai? \"\n",
    "question = \"Dr. Divyakirti ne fame aur popularity ko kaise dekha hai, especially jab negative incidents unse associate kiye jaate hain, even if it's a misunderstanding? Unhone is situation ko handle karne ke liye kaun sa analogy use kiya?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question) \n",
    "context_text = format_docs(retrieved_docs)\n",
    "first_timestamp = retrieved_docs[0].metadata['start'] if retrieved_docs else 0\n",
    "\n",
    "result = (prompt | model_google | parser).invoke({\n",
    "    \"context\": context_text,\n",
    "    \"question\": question,\n",
    "    \"video_id\": video_id,\n",
    "    \"seconds\": first_timestamp\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
