{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube RAG System\n",
    "## A Retrieval-Augmented Generation System for YouTube Video Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import (\n",
    "    YouTubeTranscriptApi,\n",
    "    TranscriptsDisabled,\n",
    "    NoTranscriptFound,\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from test2 import open_router_model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_classic.chains.query_constructor.base import AttributeInfo, load_query_constructor_runnable\n",
    "from langchain_community.query_constructors.chroma import ChromaTranslator\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "model_ep = HuggingFaceEndpoint(repo_id=\"XiaomiMiMo/MiMo-V2-Flash\", temperature=0.5)\n",
    "model_hf = ChatHuggingFace(llm=model_ep)\n",
    "model_ollama = ChatOllama(model=\"llama3.2:3b\", temperature=0.5)\n",
    "model_google = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = model_hf.invoke(\"Hello, how are you?\")\n",
    "# print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1a - Indexing (Document Ingestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Supported Languages and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major languages supported by BGE-M3\n",
    "SUPPORTED_LANGS = [\n",
    "    'en', 'hi', 'es', 'fr', 'de', 'zh-Hans', 'zh-Hant', \n",
    "    'ja', 'ko', 'ru', 'pt', 'it', 'ar', 'tr', 'vi'\n",
    "]\n",
    "\n",
    "def extract_video_id(url):\n",
    "    pattern = r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract YouTube Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_link = input(\"Paste YouTube Link: \")\n",
    "# video_id = extract_video_id(user_link)\n",
    "video_id = \"Gfr50f6ZBvo\"\n",
    "\n",
    "api = YouTubeTranscriptApi()\n",
    "try:\n",
    "    transcript_list = api.fetch(video_id, languages=SUPPORTED_LANGS)\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available for this video.\")\n",
    "except NoTranscriptFound:\n",
    "    print(\"No transcript found for this video.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching transcript: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1b - Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Transcript Fragments to Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each transcript fragment to a Document with its exact timestamp\n",
    "fragment_docs = []\n",
    "for fragment in transcript_list:\n",
    "    fragment_docs.append(\n",
    "        Document(\n",
    "            page_content=fragment.text,\n",
    "            metadata={\n",
    "                \"video_id\": video_id,\n",
    "                \"start\": int(fragment.start),\n",
    "                \"end\": int(fragment.start + fragment.duration),\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Fragments While Preserving Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transcript Length: 130047\n",
      "Chunks created: 214\n",
      "Chunk size: 1200\n",
      "Dynamic k: 10\n"
     ]
    }
   ],
   "source": [
    "total_chars = sum(len(d.page_content) for d in fragment_docs)\n",
    "print(f\"Total Transcript Length: {total_chars}\")\n",
    "\n",
    "if not fragment_docs:\n",
    "    chunks = []\n",
    "else:\n",
    "    # We never go below 600 (too fragmented) or above 1200 (too imprecise)\n",
    "    # This keeps the \"start\" timestamp within ~45-80 seconds of the actual answer.\n",
    "    target_size = max(600, min(1200, int(total_chars / 50))) \n",
    "    overlap = int(target_size * 0.15) # 15% overlap\n",
    "\n",
    "    # --- STEP B: Smart Merging with Text Anchors ---\n",
    "    raw_docs = []\n",
    "    current_content = []\n",
    "    current_start = 0\n",
    "    current_len = 0\n",
    "\n",
    "    for i, doc in enumerate(fragment_docs):\n",
    "        if not current_content:\n",
    "            current_start = doc.metadata.get(\"start\", 0)\n",
    "        \n",
    "        # Add the fragment text\n",
    "        current_content.append(doc.page_content)\n",
    "        current_len += len(doc.page_content)\n",
    "        current_end = doc.metadata.get(\"end\", 0)\n",
    "\n",
    "        # Merge until we hit the dynamic target_size\n",
    "        if current_len >= target_size or i == len(fragment_docs) - 1:\n",
    "            # Inject timestamp into text for LLM precision\n",
    "            timestamp_label = f\"[Timestamp: {int(current_start)}s] \"\n",
    "            \n",
    "            raw_docs.append(\n",
    "                Document(\n",
    "                    page_content=timestamp_label + \" \".join(current_content),\n",
    "                    metadata={\n",
    "                        \"video_id\": globals().get(\"video_id\", None),\n",
    "                        \"start\": int(current_start),\n",
    "                        \"end\": int(current_end),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            current_content = []\n",
    "            current_len = 0\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=target_size, \n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "    \n",
    "    num_chunks = len(chunks)\n",
    "    if num_chunks < 20:\n",
    "        dynamic_k = min(num_chunks, 5) # Small video: show almost everything\n",
    "    else:\n",
    "        # Long video: Scale k between 5 and 10 based on video size\n",
    "        dynamic_k = max(5, min(10, int(math.log2(num_chunks) * 1.5)))\n",
    "\n",
    "    print(f\"Chunks created: {num_chunks}\")\n",
    "    print(f\"Chunk size: {target_size}\")\n",
    "    print(f\"Dynamic k: {dynamic_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154790"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_cont = \"/n/n\".join(c.page_content for c in chunks)\n",
    "len(page_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "\n",
    "# vector store\n",
    "vector_store = Chroma.from_documents(\n",
    "    chunks, \n",
    "    embeddings, \n",
    "    collection_name=\"youtube-transcript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a brief video summary for generic prompt usage\n",
    "try:\n",
    "    sample_text = \" \".join([c.page_content for c in chunks[:10]])\n",
    "    video_summary_obj = model_ollama.invoke(\n",
    "        f\"Summarize what this video is about in one short sentence based on this text: {sample_text}\"\n",
    "    )\n",
    "    video_summary = video_summary_obj.content\n",
    "except Exception:\n",
    "    video_summary = \"this video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieval (Query Constructor) Using Self-Querying Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Metadata Field Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"start\",\n",
    "        description=\"The start time of the video segment in seconds (integer). \"\n",
    "        \"Rule 1: If user asks for 'at 12:00', use (start <= 720). \"\n",
    "        \"Rule 2: If user asks for 'after 12:00', use (start >= 720). \"\n",
    "        \"Rule 3: ALWAYS convert minutes:seconds to total seconds (min * 60 + sec). \"\n",
    "        \"Rule 4: Remove time-related keywords (e.g., '12:00', 'minutes', 'seconds') from the semantic search query part.\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"end\",\n",
    "        description=\"The end time of the video segment in seconds (integer). \"\n",
    "        \"Rule 1: If user asks for 'at 12:00', use (end >= 720).\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"video_id\",\n",
    "        description=\"The unique YouTube video identifier.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Transcript segments from a YouTube video\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Query Constructor and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the runnable with the custom prompt\n",
    "query_constructor = load_query_constructor_runnable(\n",
    "    llm=model_hf,\n",
    "    document_contents=document_content_description,\n",
    "    attribute_info=metadata_field_info,\n",
    ")\n",
    "\n",
    "# num_chunks = len(chunks)\n",
    "# print(f\"Number of chunks: {num_chunks}\")\n",
    "# # `dynamic k` which can be adjusted based on the number of chunks automatically\n",
    "# dynamic_k = max(4, min(8, num_chunks // 10))\n",
    "# print(f\"Dynamic k: {dynamic_k}\")\n",
    "\n",
    "# Initialize Retriever\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=query_constructor,\n",
    "    vectorstore=vector_store,\n",
    "    structured_query_translator=ChromaTranslator(),\n",
    "    search_kwargs={\"k\": dynamic_k},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Augmentation (Formatting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Document Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting retrieved docs with timestamps so that llm can understand the context with time\n",
    "def format_docs(retrieved_docs):\n",
    "    context_entries = []\n",
    "    for doc in retrieved_docs:\n",
    "        s = doc.metadata['start']\n",
    "        timestamp = f\"{s // 60}:{s % 60:02d}\"\n",
    "        context_entries.append(f\"[{timestamp}]: {doc.page_content}\")\n",
    "    return \"\\n\\n\".join(context_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing intent for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Intent Router \n",
    "class Router(BaseModel):\n",
    "    route: Literal[\"SUMMARY\", \"RAG\"] = Field(\n",
    "        description=\"The user's intent: 'SUMMARY' for broad overviews, 'RAG' for specific questions or greetings.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(query, history):\n",
    "    # sending llm the last 2 messages\n",
    "    recent_history = \"\\n\".join(\n",
    "        [f\"{m.type}: {m.content}\" for m in history.messages[-2:]]\n",
    "    )\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=Router)\n",
    "\n",
    "    router_instruction = \"\"\"\n",
    "    You are an expert query router. Based on the conversation history and the new user request, \n",
    "    determine if the user wants a broad overview (SUMMARY) or a specific detail/follow-up (RAG).\n",
    "\n",
    "    CONVERSATION HISTORY:\n",
    "    {history}\n",
    "\n",
    "    NEW REQUEST: \n",
    "    {query}\n",
    "\n",
    "    Rules:\n",
    "    - If the request is a follow-up to a previous specific point or asks for specific details, pick RAG.\n",
    "    - If the request is a greeting like \"hi\" or \"hello\", pick RAG.\n",
    "    - If the request asks for a general overview of the whole video, pick SUMMARY.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=router_instruction,\n",
    "        input_variables=[\"history\", \"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | model_hf | parser\n",
    "\n",
    "    try:\n",
    "        intent_obj = chain.invoke({\"history\": recent_history, \"query\": query})\n",
    "        return intent_obj.route\n",
    "    except Exception as e:\n",
    "        print(f\"Router parsing error: {e}. Defaulting to RAG.\")\n",
    "        return \"RAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart Summary Stuffing or Sampling\n",
    "If the yt transcript is greater than 500,000 characters, it takes every 2nd or 3rd chunk to stay in the high-accuracy zone else it uses the full transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Function \n",
    "def get_universal_summary(chunks): \n",
    "    MAX_CHARS = 500000 \n",
    "    # In tokens it is equivalent to MAX_CHARS / 4 (1 token = 4 characters)\n",
    "    \n",
    "    total_text = \" \".join([c.page_content for c in chunks])\n",
    "    \n",
    "    if len(total_text) > MAX_CHARS:\n",
    "        print(f\"Video is massive ({len(total_text)} chars). Using Smart Sampling...\")\n",
    "        # it takes every 2nd or 3rd chunk to stay in the high-accuracy zone\n",
    "        step = len(total_text) // MAX_CHARS + 1\n",
    "        sampled_chunks = chunks[::step] \n",
    "        final_text = \" \".join([c.page_content for c in sampled_chunks])\n",
    "    else:\n",
    "        print(\"Video is standard size. Using full transcript...\")\n",
    "        final_text = total_text\n",
    "\n",
    "    res = model_hf.invoke(f\"\"\"\n",
    "        Summarize this YouTube video professionally. \n",
    "        Provide a concise 4-5 sentence overview followed by key takeaways in bullet points and mention all the key topics covered in the video.\n",
    "        Don't include the youtube source link in the summary.\n",
    "        VIDEO CONTENT:\n",
    "        {final_text}\n",
    "    \"\"\")\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful YouTube AI assistant. \n",
    "    PRIMARY TASK:\n",
    "    - Your primary goal is to answer the [USER QUESTION] provided below.\n",
    "    - Use the [CHAT HISTORY] ONLY for context (e.g., if the user refers to a previous point). \n",
    "    - DO NOT answer old questions from the chat history.\n",
    "\n",
    "    VIDEO CONTENT:\n",
    "    {context}\n",
    "\n",
    "    CHAT HISTORY:\n",
    "    {chat_history}\n",
    "\n",
    "    USER QUESTION: \n",
    "    {question}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. GENERAL CONVERSATION & GREETINGS:\n",
    "       - Reply naturally and warmly.\n",
    "       - Acknowledge that you are here to help with the video. You can mention that the video is about: {video_summary}\n",
    "       - DO NOT include Source Links or timestamps for general chat.\n",
    "\n",
    "    2. TIMESTAMP QUERIES:\n",
    "       - If the user asks about a specific time (e.g., \"at 54:00\"), use the closest available segments in the [VIDEO CONTENT].\n",
    "       - Answer based on that content naturally. Simply state what is being discussed in that portion of the video.\n",
    "       - CRITICAL: DO NOT expose technical details like \"the transcript only includes segments from X\" or mention specific timestamp ranges you have access to.\n",
    "       - If the content around that time isn't available, just say: \"I couldn't find specific information about that timestamp in the video.\"\n",
    "\n",
    "    3. VIDEO QUESTIONS (INFORMATION FOUND):\n",
    "       - Answer using ONLY the [VIDEO CONTENT] provided.\n",
    "       - You MUST append the source link at the end of your response: https://youtu.be/{video_id}?t={seconds}s\n",
    "       - Use the 'seconds' variable provided to you for the link.\n",
    "\n",
    "    4. VIDEO QUESTIONS (INFORMATION NOT FOUND):\n",
    "       - If the user asks about something not in the video, politely say: \"I couldn't find information about that in this video.\"\n",
    "       - Briefly mention the general theme: \"This video focuses on {video_summary}. Would you like to know about that instead?\"\n",
    "       - DO NOT expose internal details about what segments or timestamps you have access to.\n",
    "       - DO NOT provide a source link if the answer is not found.\n",
    "\n",
    "    5. PERSONAL OPINIONS:\n",
    "       - If the user asks for YOUR view or opinion, start by saying: \"As an AI assistant, I don't have personal opinions. However, based on the video content...\" and then proceed to answer using the transcript content.\n",
    "\n",
    "    6. FORMATTING:\n",
    "       - Keep responses conversational, helpful, and grounded.\n",
    "       - Never expose technical implementation details to the user.\n",
    "       - Respond in the same language as the [USER QUESTION].\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\",\"question\",\"video_id\",\"seconds\",\"chat_history\",\"video_summary\",],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Chat History and Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Video processed. I'm ready!\n",
      "\n",
      "User: hii how can u help me?\n",
      "\n",
      "AI (RAG mode): Hi there! I'd be happy to help you!\n",
      "\n",
      "I'm here to assist you with this video featuring Demis Hassabis, CEO and co-founder of DeepMind. In the video, he discusses the Turing Test, its limitations, and how he believes true AI capabilities will be demonstrated through generalizability across multiple tasks rather than just language-based communication.\n",
      "\n",
      "How can I help you with the video? Do you have any questions about what's being discussed?\n",
      "\n",
      "User: What game does Demis consider the most impressive example of reinforcement learning in a computer game, and what was its core mechanic?\n",
      "\n",
      "AI (RAG mode): Hey there! I'm here to help you with the video featuring Demis Hassabis, CEO and co-founder of DeepMind. In the video, he discusses the Turing Test, its limitations, and how he believes true AI capabilities will be demonstrated through generalizability across multiple tasks rather than just language-based communication.\n",
      "\n",
      "Regarding your question: Demis considers **Black and White** the most impressive example of reinforcement learning in a computer game. The core mechanic involved training a little pet animal that learned from how you treated it - if you treated it badly, it became mean and would be mean to your villagers, but if you were kind to it, it would be kind in return.\n",
      "\n",
      "https://youtu.be/Gfr50f6ZBvo?t=918s\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "parser = StrOutputParser()\n",
    "rag_chain = prompt | model_hf | parser\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "summary_cache = None \n",
    "\n",
    "print(\"\\nAI: Video processed. I'm ready!\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nUser: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "         break\n",
    "\n",
    "    intent = get_intent(query, history)\n",
    "      \n",
    "    if intent == \"SUMMARY\":\n",
    "        if not summary_cache:\n",
    "            summary_cache = get_universal_summary(chunks)\n",
    "        context_text = summary_cache\n",
    "        timestamp = 0\n",
    "    else:\n",
    "        # RAG Mode\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        # Use the most relevant chunk's timestamp for the source link\n",
    "        sorted_docs = sorted(retrieved_docs, key=lambda x: x.metadata.get(\"start\", 0))\n",
    "        context_text = \"\\n\\n\".join([f\"[{d.metadata['start']}s]: {d.page_content}\" for d in sorted_docs])\n",
    "        timestamp = retrieved_docs[0].metadata[\"start\"] if retrieved_docs else 0\n",
    "\n",
    "    # CALL 3: Final Answer Generation (keeping last 6 messages in the chat history)\n",
    "    history_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in history.messages[-6:]])\n",
    "\n",
    "    result = rag_chain.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"question\": query,\n",
    "        \"video_id\": video_id,\n",
    "        \"seconds\": timestamp,\n",
    "        \"chat_history\": history_str,\n",
    "        \"video_summary\": video_summary \n",
    "    })\n",
    "\n",
    "    history.add_user_message(query)\n",
    "    history.add_ai_message(result)\n",
    "    \n",
    "    print(\"\\nUser: \" + query)\n",
    "    print(f\"\\nAI ({intent} mode): {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# question = \"is the topic of origin of life discussed in this video? if yes then what was discussed\"\n",
    "# question = \"What does Demis say about aliens or life on other planets?\"\n",
    "# question = \"What is the fundamental problem that AlphaFold solved, and why is it significant for biology and medicine?\"\n",
    "# question = \"Who first articulated the protein folding problem, and when?\"\n",
    "# question = \"Does Demis Hassabis believe we are living in a computer game-like simulation, and what is his alternative view on understanding the universe?\"\n",
    "# question = \"What game does Demis consider the most impressive example of reinforcement learning in a computer game, and what was its core mechanic?\"\n",
    "# question = \"What specific topic is Demis Hassabis discussing right at (34:00) in the video?\"\n",
    "# question = \"Dr. Divyakirti ne evolutionary psychology ka use karke, men mein emotions suppress karne ke phenomenon ko kaise explain kiya hai?\"\n",
    "# question = \"Video mein, Dr. Divyakirti ne poverty scar hypothesis ke baare mein kya bataya? Aur unka personal experience is hypothesis se kaise align karta hai ya differ karta hai? \"\n",
    "question = \"Dr. Divyakirti ne fame aur popularity ko kaise dekha hai, especially jab negative incidents unse associate kiye jaate hain, even if it's a misunderstanding? Unhone is situation ko handle karne ke liye kaun sa analogy use kiya?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
